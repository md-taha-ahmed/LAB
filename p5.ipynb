{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5- Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using appropriate data sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "iteration: 0 :::: [[ 0.10719494 -0.8386037 ]]\n",
      "###output######## [[0.89280506 0.8386037 ]]\n",
      "**********************\n",
      "iteration: 1 :::: [[ 0.1071061  -0.83670232]]\n",
      "###output######## [[0.8928939  0.83670232]]\n",
      "**********************\n",
      "iteration: 2 :::: [[ 0.10701759 -0.83477056]]\n",
      "###output######## [[0.89298241 0.83477056]]\n",
      "**********************\n",
      "iteration: 3 :::: [[ 0.1069294  -0.83280801]]\n",
      "###output######## [[0.8930706  0.83280801]]\n",
      "**********************\n",
      "iteration: 4 :::: [[ 0.10684152 -0.83081423]]\n",
      "###output######## [[0.89315848 0.83081423]]\n",
      "**********************\n",
      "iteration: 5 :::: [[ 0.10675395 -0.82878881]]\n",
      "###output######## [[0.89324605 0.82878881]]\n",
      "**********************\n",
      "iteration: 6 :::: [[ 0.10666668 -0.82673132]]\n",
      "###output######## [[0.89333332 0.82673132]]\n",
      "**********************\n",
      "iteration: 7 :::: [[ 0.10657971 -0.82464135]]\n",
      "###output######## [[0.89342029 0.82464135]]\n",
      "**********************\n",
      "iteration: 8 :::: [[ 0.10649302 -0.82251851]]\n",
      "###output######## [[0.89350698 0.82251851]]\n",
      "**********************\n",
      "iteration: 9 :::: [[ 0.10640661 -0.82036237]]\n",
      "###output######## [[0.89359339 0.82036237]]\n",
      "**********************\n",
      "iteration: 10 :::: [[ 0.10632048 -0.81817255]]\n",
      "###output######## [[0.89367952 0.81817255]]\n",
      "**********************\n",
      "iteration: 11 :::: [[ 0.1062346  -0.81594867]]\n",
      "###output######## [[0.8937654  0.81594867]]\n",
      "**********************\n",
      "iteration: 12 :::: [[ 0.10614899 -0.81369033]]\n",
      "###output######## [[0.89385101 0.81369033]]\n",
      "**********************\n",
      "iteration: 13 :::: [[ 0.10606362 -0.81139718]]\n",
      "###output######## [[0.89393638 0.81139718]]\n",
      "**********************\n",
      "iteration: 14 :::: [[ 0.10597849 -0.80906885]]\n",
      "###output######## [[0.89402151 0.80906885]]\n",
      "**********************\n",
      "iteration: 15 :::: [[ 0.10589359 -0.806705  ]]\n",
      "###output######## [[0.89410641 0.806705  ]]\n",
      "**********************\n",
      "iteration: 16 :::: [[ 0.10580891 -0.80430529]]\n",
      "###output######## [[0.89419109 0.80430529]]\n",
      "**********************\n",
      "iteration: 17 :::: [[ 0.10572445 -0.8018694 ]]\n",
      "###output######## [[0.89427555 0.8018694 ]]\n",
      "**********************\n",
      "iteration: 18 :::: [[ 0.10564019 -0.79939703]]\n",
      "###output######## [[0.89435981 0.79939703]]\n",
      "**********************\n",
      "iteration: 19 :::: [[ 0.10555613 -0.79688788]]\n",
      "###output######## [[0.89444387 0.79688788]]\n",
      "**********************\n",
      "iteration: 20 :::: [[ 0.10547226 -0.79434169]]\n",
      "###output######## [[0.89452774 0.79434169]]\n",
      "**********************\n",
      "iteration: 21 :::: [[ 0.10538857 -0.79175819]]\n",
      "###output######## [[0.89461143 0.79175819]]\n",
      "**********************\n",
      "iteration: 22 :::: [[ 0.10530504 -0.78913716]]\n",
      "###output######## [[0.89469496 0.78913716]]\n",
      "**********************\n",
      "iteration: 23 :::: [[ 0.10522168 -0.78647838]]\n",
      "###output######## [[0.89477832 0.78647838]]\n",
      "**********************\n",
      "iteration: 24 :::: [[ 0.10513846 -0.78378166]]\n",
      "###output######## [[0.89486154 0.78378166]]\n",
      "**********************\n",
      "iteration: 25 :::: [[ 0.10505538 -0.78104683]]\n",
      "###output######## [[0.89494462 0.78104683]]\n",
      "**********************\n",
      "iteration: 26 :::: [[ 0.10497244 -0.77827375]]\n",
      "###output######## [[0.89502756 0.77827375]]\n",
      "**********************\n",
      "iteration: 27 :::: [[ 0.10488961 -0.7754623 ]]\n",
      "###output######## [[0.89511039 0.7754623 ]]\n",
      "**********************\n",
      "iteration: 28 :::: [[ 0.1048069  -0.77261239]]\n",
      "###output######## [[0.8951931  0.77261239]]\n",
      "**********************\n",
      "iteration: 29 :::: [[ 0.10472429 -0.76972396]]\n",
      "###output######## [[0.89527571 0.76972396]]\n",
      "**********************\n",
      "iteration: 30 :::: [[ 0.10464177 -0.76679697]]\n",
      "###output######## [[0.89535823 0.76679697]]\n",
      "**********************\n",
      "iteration: 31 :::: [[ 0.10455933 -0.76383144]]\n",
      "###output######## [[0.89544067 0.76383144]]\n",
      "**********************\n",
      "iteration: 32 :::: [[ 0.10447695 -0.76082739]]\n",
      "###output######## [[0.89552305 0.76082739]]\n",
      "**********************\n",
      "iteration: 33 :::: [[ 0.10439464 -0.75778489]]\n",
      "###output######## [[0.89560536 0.75778489]]\n",
      "**********************\n",
      "iteration: 34 :::: [[ 0.10431238 -0.75470404]]\n",
      "###output######## [[0.89568762 0.75470404]]\n",
      "**********************\n",
      "iteration: 35 :::: [[ 0.10423016 -0.75158497]]\n",
      "###output######## [[0.89576984 0.75158497]]\n",
      "**********************\n",
      "iteration: 36 :::: [[ 0.10414797 -0.74842787]]\n",
      "###output######## [[0.89585203 0.74842787]]\n",
      "**********************\n",
      "iteration: 37 :::: [[ 0.1040658  -0.74523294]]\n",
      "###output######## [[0.8959342  0.74523294]]\n",
      "**********************\n",
      "iteration: 38 :::: [[ 0.10398363 -0.74200044]]\n",
      "###output######## [[0.89601637 0.74200044]]\n",
      "**********************\n",
      "iteration: 39 :::: [[ 0.10390147 -0.73873066]]\n",
      "###output######## [[0.89609853 0.73873066]]\n",
      "**********************\n",
      "iteration: 40 :::: [[ 0.10381929 -0.73542394]]\n",
      "###output######## [[0.89618071 0.73542394]]\n",
      "**********************\n",
      "iteration: 41 :::: [[ 0.10373709 -0.73208065]]\n",
      "###output######## [[0.89626291 0.73208065]]\n",
      "**********************\n",
      "iteration: 42 :::: [[ 0.10365486 -0.7287012 ]]\n",
      "###output######## [[0.89634514 0.7287012 ]]\n",
      "**********************\n",
      "iteration: 43 :::: [[ 0.10357259 -0.72528607]]\n",
      "###output######## [[0.89642741 0.72528607]]\n",
      "**********************\n",
      "iteration: 44 :::: [[ 0.10349027 -0.72183575]]\n",
      "###output######## [[0.89650973 0.72183575]]\n",
      "**********************\n",
      "iteration: 45 :::: [[ 0.10340789 -0.71835079]]\n",
      "###output######## [[0.89659211 0.71835079]]\n",
      "**********************\n",
      "iteration: 46 :::: [[ 0.10332543 -0.7148318 ]]\n",
      "###output######## [[0.89667457 0.7148318 ]]\n",
      "**********************\n",
      "iteration: 47 :::: [[ 0.1032429  -0.71127939]]\n",
      "###output######## [[0.8967571  0.71127939]]\n",
      "**********************\n",
      "iteration: 48 :::: [[ 0.10316028 -0.70769426]]\n",
      "###output######## [[0.89683972 0.70769426]]\n",
      "**********************\n",
      "iteration: 49 :::: [[ 0.10307756 -0.70407714]]\n",
      "###output######## [[0.89692244 0.70407714]]\n",
      "**********************\n",
      "iteration: 5951 :::: [[ 0.02442373 -0.02566183]]\n",
      "###output######## [[0.97557627 0.02566183]]\n",
      "**********************\n",
      "iteration: 5952 :::: [[ 0.02442163 -0.02565938]]\n",
      "###output######## [[0.97557837 0.02565938]]\n",
      "**********************\n",
      "iteration: 5953 :::: [[ 0.02441953 -0.02565693]]\n",
      "###output######## [[0.97558047 0.02565693]]\n",
      "**********************\n",
      "iteration: 5954 :::: [[ 0.02441743 -0.02565448]]\n",
      "###output######## [[0.97558257 0.02565448]]\n",
      "**********************\n",
      "iteration: 5955 :::: [[ 0.02441533 -0.02565203]]\n",
      "###output######## [[0.97558467 0.02565203]]\n",
      "**********************\n",
      "iteration: 5956 :::: [[ 0.02441324 -0.02564958]]\n",
      "###output######## [[0.97558676 0.02564958]]\n",
      "**********************\n",
      "iteration: 5957 :::: [[ 0.02441114 -0.02564713]]\n",
      "###output######## [[0.97558886 0.02564713]]\n",
      "**********************\n",
      "iteration: 5958 :::: [[ 0.02440905 -0.02564469]]\n",
      "###output######## [[0.97559095 0.02564469]]\n",
      "**********************\n",
      "iteration: 5959 :::: [[ 0.02440695 -0.02564224]]\n",
      "###output######## [[0.97559305 0.02564224]]\n",
      "**********************\n",
      "iteration: 5960 :::: [[ 0.02440486 -0.0256398 ]]\n",
      "###output######## [[0.97559514 0.0256398 ]]\n",
      "**********************\n",
      "iteration: 5961 :::: [[ 0.02440276 -0.02563735]]\n",
      "###output######## [[0.97559724 0.02563735]]\n",
      "**********************\n",
      "iteration: 5962 :::: [[ 0.02440067 -0.02563491]]\n",
      "###output######## [[0.97559933 0.02563491]]\n",
      "**********************\n",
      "iteration: 5963 :::: [[ 0.02439857 -0.02563246]]\n",
      "###output######## [[0.97560143 0.02563246]]\n",
      "**********************\n",
      "iteration: 5964 :::: [[ 0.02439648 -0.02563002]]\n",
      "###output######## [[0.97560352 0.02563002]]\n",
      "**********************\n",
      "iteration: 5965 :::: [[ 0.02439439 -0.02562758]]\n",
      "###output######## [[0.97560561 0.02562758]]\n",
      "**********************\n",
      "iteration: 5966 :::: [[ 0.0243923  -0.02562514]]\n",
      "###output######## [[0.9756077  0.02562514]]\n",
      "**********************\n",
      "iteration: 5967 :::: [[ 0.02439021 -0.0256227 ]]\n",
      "###output######## [[0.97560979 0.0256227 ]]\n",
      "**********************\n",
      "iteration: 5968 :::: [[ 0.02438812 -0.02562026]]\n",
      "###output######## [[0.97561188 0.02562026]]\n",
      "**********************\n",
      "iteration: 5969 :::: [[ 0.02438603 -0.02561782]]\n",
      "###output######## [[0.97561397 0.02561782]]\n",
      "**********************\n",
      "iteration: 5970 :::: [[ 0.02438394 -0.02561538]]\n",
      "###output######## [[0.97561606 0.02561538]]\n",
      "**********************\n",
      "iteration: 5971 :::: [[ 0.02438185 -0.02561294]]\n",
      "###output######## [[0.97561815 0.02561294]]\n",
      "**********************\n",
      "iteration: 5972 :::: [[ 0.02437976 -0.0256105 ]]\n",
      "###output######## [[0.97562024 0.0256105 ]]\n",
      "**********************\n",
      "iteration: 5973 :::: [[ 0.02437767 -0.02560807]]\n",
      "###output######## [[0.97562233 0.02560807]]\n",
      "**********************\n",
      "iteration: 5974 :::: [[ 0.02437558 -0.02560563]]\n",
      "###output######## [[0.97562442 0.02560563]]\n",
      "**********************\n",
      "iteration: 5975 :::: [[ 0.0243735 -0.0256032]]\n",
      "###output######## [[0.9756265 0.0256032]]\n",
      "**********************\n",
      "iteration: 5976 :::: [[ 0.02437141 -0.02560076]]\n",
      "###output######## [[0.97562859 0.02560076]]\n",
      "**********************\n",
      "iteration: 5977 :::: [[ 0.02436932 -0.02559833]]\n",
      "###output######## [[0.97563068 0.02559833]]\n",
      "**********************\n",
      "iteration: 5978 :::: [[ 0.02436724 -0.02559589]]\n",
      "###output######## [[0.97563276 0.02559589]]\n",
      "**********************\n",
      "iteration: 5979 :::: [[ 0.02436515 -0.02559346]]\n",
      "###output######## [[0.97563485 0.02559346]]\n",
      "**********************\n",
      "iteration: 5980 :::: [[ 0.02436307 -0.02559103]]\n",
      "###output######## [[0.97563693 0.02559103]]\n",
      "**********************\n",
      "iteration: 5981 :::: [[ 0.02436099 -0.0255886 ]]\n",
      "###output######## [[0.97563901 0.0255886 ]]\n",
      "**********************\n",
      "iteration: 5982 :::: [[ 0.0243589  -0.02558617]]\n",
      "###output######## [[0.9756411  0.02558617]]\n",
      "**********************\n",
      "iteration: 5983 :::: [[ 0.02435682 -0.02558374]]\n",
      "###output######## [[0.97564318 0.02558374]]\n",
      "**********************\n",
      "iteration: 5984 :::: [[ 0.02435474 -0.02558131]]\n",
      "###output######## [[0.97564526 0.02558131]]\n",
      "**********************\n",
      "iteration: 5985 :::: [[ 0.02435266 -0.02557888]]\n",
      "###output######## [[0.97564734 0.02557888]]\n",
      "**********************\n",
      "iteration: 5986 :::: [[ 0.02435057 -0.02557645]]\n",
      "###output######## [[0.97564943 0.02557645]]\n",
      "**********************\n",
      "iteration: 5987 :::: [[ 0.02434849 -0.02557402]]\n",
      "###output######## [[0.97565151 0.02557402]]\n",
      "**********************\n",
      "iteration: 5988 :::: [[ 0.02434641 -0.0255716 ]]\n",
      "###output######## [[0.97565359 0.0255716 ]]\n",
      "**********************\n",
      "iteration: 5989 :::: [[ 0.02434433 -0.02556917]]\n",
      "###output######## [[0.97565567 0.02556917]]\n",
      "**********************\n",
      "iteration: 5990 :::: [[ 0.02434225 -0.02556675]]\n",
      "###output######## [[0.97565775 0.02556675]]\n",
      "**********************\n",
      "iteration: 5991 :::: [[ 0.02434018 -0.02556432]]\n",
      "###output######## [[0.97565982 0.02556432]]\n",
      "**********************\n",
      "iteration: 5992 :::: [[ 0.0243381 -0.0255619]]\n",
      "###output######## [[0.9756619 0.0255619]]\n",
      "**********************\n",
      "iteration: 5993 :::: [[ 0.02433602 -0.02555947]]\n",
      "###output######## [[0.97566398 0.02555947]]\n",
      "**********************\n",
      "iteration: 5994 :::: [[ 0.02433394 -0.02555705]]\n",
      "###output######## [[0.97566606 0.02555705]]\n",
      "**********************\n",
      "iteration: 5995 :::: [[ 0.02433187 -0.02555463]]\n",
      "###output######## [[0.97566813 0.02555463]]\n",
      "**********************\n",
      "iteration: 5996 :::: [[ 0.02432979 -0.02555221]]\n",
      "###output######## [[0.97567021 0.02555221]]\n",
      "**********************\n",
      "iteration: 5997 :::: [[ 0.02432771 -0.02554979]]\n",
      "###output######## [[0.97567229 0.02554979]]\n",
      "**********************\n",
      "iteration: 5998 :::: [[ 0.02432564 -0.02554736]]\n",
      "###output######## [[0.97567436 0.02554736]]\n",
      "**********************\n",
      "iteration: 5999 :::: [[ 0.02432356 -0.02554495]]\n",
      "###output######## [[0.97567644 0.02554495]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputNeurons=2\n",
    "hiddenlayerNeurons=4\n",
    "outputNeurons=2\n",
    "iteration=6000\n",
    "\n",
    "input = np.random.randint(1,5,inputNeurons)\n",
    "output = np.array([1.0,0.0])\n",
    "hidden_layer=np.random.rand(1,hiddenlayerNeurons)\n",
    "\n",
    "hidden_biass=np.random.rand(1,hiddenlayerNeurons)\n",
    "output_bias=np.random.rand(1,outputNeurons)\n",
    "hidden_weights=np.random.rand(inputNeurons,hiddenlayerNeurons)\n",
    "output_weights=np.random.rand(hiddenlayerNeurons,outputNeurons)\n",
    "\n",
    "def sigmoid (layer):\n",
    "    return 1/(1 + np.exp(-layer))\n",
    "\n",
    "def gradient(layer):\n",
    "    return layer*(1-layer)\n",
    "\n",
    "for i in range(iteration):\n",
    "\n",
    "    hidden_layer=np.dot(input,hidden_weights)\n",
    "    hidden_layer=sigmoid(hidden_layer+hidden_biass)\n",
    "\n",
    "    output_layer=np.dot(hidden_layer,output_weights)\n",
    "    output_layer=sigmoid(output_layer+output_bias)\n",
    "\n",
    "    error = (output-output_layer)\n",
    "    gradient_outputLayer=gradient(output_layer)\n",
    "    error_terms_output=gradient_outputLayer * error\n",
    "    error_terms_hidden=gradient(hidden_layer)*np.dot(error_terms_output,output_weights.T)\n",
    "\n",
    "    gradient_hidden_weights = np.dot(input.reshape(inputNeurons,1),error_terms_hidden.reshape(1,hiddenlayerNeurons))\n",
    "    gradient_ouput_weights = np.dot(hidden_layer.reshape(hiddenlayerNeurons,1),error_terms_output.reshape(1,outputNeurons))\n",
    "\n",
    "    hidden_weights = hidden_weights + 0.05*gradient_hidden_weights\n",
    "    output_weights = output_weights + 0.05*gradient_ouput_weights\n",
    "    \n",
    "    if i<50 or i>iteration-50:\n",
    "        print(\"**********************\")\n",
    "        print(\"iteration:\",i,\"::::\",error)\n",
    "        print(\"###output########\",output_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
